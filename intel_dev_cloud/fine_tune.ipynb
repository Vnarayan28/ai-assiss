{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Installation in progress, please wait...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://developer.intel.com/ipex-whl-stable-xpu\n",
      "Collecting bigdl-llm==2.5.0b20240318 (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached bigdl_llm-2.5.0b20240318-py3-none-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\vedan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bigdl-llm[xpu]==2.5.0b20240318) (9.0.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\vedan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bigdl-llm[xpu]==2.5.0b20240318) (5.29.3)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\vedan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bigdl-llm[xpu]==2.5.0b20240318) (1.3.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\vedan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bigdl-llm[xpu]==2.5.0b20240318) (1.26.4)\n",
      "Collecting transformers==4.31.0 (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Collecting sentencepiece (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers==0.13.3 (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate==0.21.0 (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting tabulate (from bigdl-llm[xpu]==2.5.0b20240318)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "INFO: pip is looking at multiple versions of bigdl-llm[xpu] to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.0a0; extra == \"xpu\" (from bigdl-llm[xpu]) (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torch==2.1.0a0; extra == \"xpu\"\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Installation completed.\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import site\n",
    "from pathlib import Path\n",
    "\n",
    "!echo \"Installation in progress, please wait...\"\n",
    "!{sys.executable} -m pip cache purge > /dev/null\n",
    "!{sys.executable} -m pip install --pre --upgrade \"bigdl-llm[xpu]==2.5.0b20240318\" -f https://developer.intel.com/ipex-whl-stable-xpu\n",
    "!{sys.executable} -m pip install \"peft==0.10.0\"  #> /dev/null\n",
    "!{sys.executable} -m pip install \"accelerate==0.27.2\" --no-warn-script-location #> /dev/null\n",
    "!{sys.executable} -m pip install \"transformers==4.40.0\" --no-warn-script-location #> /dev/null \n",
    "!{sys.executable} -m pip install \"datasets==2.19.0\" --no-warn-script-location #> /dev/null 2>&1 \n",
    "!{sys.executable} -m pip install \"bitsandbytes==0.43.1\" \"scipy==1.13.0\" #> /dev/null  2>&1\n",
    "!echo \"Installation completed.\"\n",
    "\n",
    "def get_python_version():\n",
    "    return \"python\" + \".\".join(map(str, sys.version_info[:2]))\n",
    "\n",
    "def set_local_bin_path():\n",
    "    local_bin = str(Path.home() / \".local\" / \"bin\") \n",
    "    local_site_packages = str(\n",
    "        Path.home() / \".local\" / \"lib\" / get_python_version() / \"site-packages\"\n",
    "    )\n",
    "\n",
    "    sys.path.append(local_bin)\n",
    "    sys.path.insert(0, site.getusersitepackages())\n",
    "\n",
    "    if local_site_packages in sys.path:\n",
    "        sys.path.insert(0, sys.path.pop(sys.path.index(local_site_packages)))\n",
    "\n",
    "set_local_bin_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement intel-extension-for-pytorch (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for intel-extension-for-pytorch\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intel_extension_for_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mintel_extension_for_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipex\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPEX version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ipex\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'intel_extension_for_pytorch'"
     ]
    }
   ],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "print(\"IPEX version:\", ipex.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intel_extension_for_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigdl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mipex\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'intel_extension_for_pytorch'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"intel_extension_for_pytorch\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"torchvision.io.image\", lineno=13\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The installed version of bitsandbytes was compiled without GPU support.*\",\n",
    "    category=UserWarning,\n",
    "    module='bitsandbytes.cextension'\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"This implementation of AdamW is deprecated\",\n",
    ")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"28\"\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"true\"\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"]=\"1\"\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bigdl\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.qlora import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training as prepare_model,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 280 GiB\n",
      "Used: 91 GiB\n",
      "Free: 189 GiB\n"
     ]
    }
   ],
   "source": [
    "# Function to check available disk space in the Hugging Face cache directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_disk_space(path=\"~/.cache/huggingface/\"):\n",
    "    abs_path = os.path.expanduser(path)\n",
    "    total, used, free = shutil.disk_usage(abs_path)\n",
    "    print(f\"Total: {total // (2**30)} GiB\")\n",
    "    print(f\"Used: {used // (2**30)} GiB\")\n",
    "    print(f\"Free: {free // (2**30)} GiB\")\n",
    "\n",
    "# Example usage\n",
    "check_disk_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Using Device: cpu\n",
      "Final model will be saved to: ./final_model\n",
      "LoRA adapters will be saved to: ./lora_adapters\n",
      "Finetuning Model: google/gemma-7b\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "\n",
    "BASE_MODEL = \"google/gemma-7b\"\n",
    "MODEL_PATH = \"./final_model\"\n",
    "ADAPTER_PATH = \"./lora_adapters\"\n",
    "DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Final model will be saved to: {MODEL_PATH}\")\n",
    "print(f\"LoRA adapters will be saved to: {ADAPTER_PATH}\")\n",
    "print(f\"Finetuning Model: {BASE_MODEL}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(base_model_id: str):\n",
    "    \"\"\"Downloads / Loads the pre-trained model and tokenizer based on the given base model ID for training, \n",
    "    with fallbacks for permission errors to use default cache.\"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    print(\"Downloading model and tokenizer...\" + base_model_id)\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id, \n",
    "            # quantization_config=bnb_config,\n",
    "            # torch_dtype=torch.float32,\n",
    "            token=os.environ.get(\"HF_TOKEN\", None),\n",
    "        )\n",
    "        tokenizer_class = LlamaTokenizer if \"llama\" in base_model_id.lower() else AutoTokenizer\n",
    "        tokenizer = tokenizer_class.from_pretrained(base_model_id,token=os.environ.get(\"HF_TOKEN\", None))\n",
    "        print(\"Downloaded model and tokenizer successfully.\")\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Error downloading model and tokenizer: {e}\")\n",
    "\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vedan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 2143.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### USER: You are an experienced Javascript engineer in charge of converting educational prompts into valid MermaidJS code. Make sure to only return valid MermaidJS code and nothing else. Prompt: How does a computer execute a program?\n",
      "### ASSISTANT: graph LR;\n",
      "            A[Start] --> B[Load Program]\n",
      "            B --> C[Execute Instructions]\n",
      "            C --> D[End]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Your data as a list of dictionaries\n",
    "data = {\n",
    "    \"input\": [\n",
    "        \"How does a computer execute a program?\",\n",
    "        \"What is the process for finding the maximum subarray sum using Kadane's Algorithm?\",\n",
    "        \"How does the Quick Sort algorithm process an array to sort its elements?\",\n",
    "        \"How does a binary search algorithm find an element in a sorted array?\",\n",
    "        \"What is the process of inserting a node into a binary search tree?\",\n",
    "        \"How does a hash table handle collisions using chaining?\",\n",
    "        \"What is the process of performing a depth-first search on a graph?\",\n",
    "        \"How does the merge sort algorithm sort an array?\"\n",
    "    ],\n",
    "    \"output\": [\n",
    "        \"\"\"graph LR;\n",
    "            A[Start] --> B[Load Program]\n",
    "            B --> C[Execute Instructions]\n",
    "            C --> D[End]\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Init max_sum, current_sum = 0]\n",
    "            B --> C[For each element]\n",
    "            C --> D[Add to current_sum]\n",
    "            D --> E[current_sum < 0?]\n",
    "            E -- Yes --> F[Reset current_sum]\n",
    "            E -- No --> G[current_sum > max_sum?]\n",
    "            G -- Yes --> H[Update max_sum]\n",
    "            G -- No --> I[Next element]\n",
    "            F --> J[End array?]\n",
    "            H --> J\n",
    "            I --> J\n",
    "            J -- No --> C\n",
    "            J -- Yes --> K[Return max_sum]\n",
    "            K --> L[End]\"\"\",\n",
    "        \"\"\"sequenceDiagram\n",
    "            participant S as Start\n",
    "            participant P as Select Pivot\n",
    "            participant PA as Partition Array\n",
    "            participant SL as Sort Left Subarray\n",
    "            participant SR as Sort Right Subarray\n",
    "            participant E as End\n",
    "\n",
    "            S->>P: Begin Sorting\n",
    "            P->>PA: Choose pivot element\n",
    "            PA->>SL: Partition left of pivot\n",
    "            SL->>SL: Recurse left\n",
    "            PA->>SR: Partition right of pivot\n",
    "            SR->>SR: Recurse right\n",
    "            SL->>E: Left sorted\n",
    "            SR->>E: Right sorted\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Initialize pointers]\n",
    "            B --> C[Check middle element]\n",
    "            C --> D{Is it the target?}\n",
    "            D -- Yes --> E[Found]\n",
    "            D -- No --> F{Is target less than middle?}\n",
    "            F -- Yes --> G[Move right pointer]\n",
    "            F -- No --> H[Move left pointer]\n",
    "            G --> C\n",
    "            H --> C\n",
    "            E --> I[End]\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Find position]\n",
    "            B --> C{Found spot?}\n",
    "            C -- Yes --> D[Insert node]\n",
    "            C -- No --> E{Is node smaller?}\n",
    "            E -- Yes --> F[Go left]\n",
    "            E -- No --> G[Go right]\n",
    "            F --> B\n",
    "            G --> B\n",
    "            D --> H[End]\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Compute hash]\n",
    "            B --> C{Collision?}\n",
    "            C -- Yes --> D[Link new entry]\n",
    "            C -- No --> E[Insert normally]\n",
    "            D --> F[End]\n",
    "            E --> F\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Mark as visited]\n",
    "            B --> C[Visit first unvisited neighbor]\n",
    "            C --> D{All neighbors visited?}\n",
    "            D -- No --> B\n",
    "            D -- Yes --> E[Backtrack]\n",
    "            E --> C\n",
    "            C --> F[End]\"\"\",\n",
    "        \"\"\"flowchart LR;\n",
    "            A[Start] --> B[Split array into halves]\n",
    "            B --> C[Sort each half]\n",
    "            C --> D[Merge halves]\n",
    "            D --> E{Is array fully sorted?}\n",
    "            E -- Yes --> F[End]\n",
    "            E -- No --> B\"\"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert list of dicts to a Hugging Face dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Function to format each entry for training\n",
    "def formatting_func(example):\n",
    "    text = f\"### USER: You are an experienced Javascript engineer in charge of converting educational prompts into valid MermaidJS code. Make sure to only return valid MermaidJS code and nothing else. Prompt: {example['input']}\\n### ASSISTANT: {example['output']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting function\n",
    "formatted_dataset = dataset.map(formatting_func)\n",
    "\n",
    "# Display an example to verify formatting\n",
    "print(formatted_dataset[0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class FineTuner:\n",
    "    \"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id: str, model_path: str, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize the FineTuner with base model, model path, and device.\n",
    "\n",
    "        Parameters:\n",
    "            base_model_id (str): Id of pre-trained model to use for fine-tuning.\n",
    "            model_path (str): Path to save the fine-tuned model.\n",
    "            device (torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "\n",
    "    def tokenize_data(self, example, add_eos_token=True, train_on_inputs=False, cutoff_len=512):\n",
    "        \"\"\"\n",
    "        Tokenizes a MermaidJS data example.\n",
    "\n",
    "        Parameters:\n",
    "            example (dict): A data example containing 'text'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = example[\"text\"]\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            if (\n",
    "                tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "            ):\n",
    "                tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            return tokenized\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in tokenization: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def prepare_data(self, dataset, val_set_size=2):\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        try:\n",
    "            train_val_split = dataset.train_test_split(test_size=val_set_size, shuffle=True, seed=42)\n",
    "            train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "            val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "            return train_data, val_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preparing data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, train_data, val_data, training_args):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with the given training and validation data.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (Dataset): Training data.\n",
    "            val_data (Optional[Dataset]): Validation data.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            self.model = prepare_model(self.model)\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    self.tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "            self.model.config.use_cache = False\n",
    "            results = trainer.train()\n",
    "            self.model.save_pretrained(self.model_path)\n",
    "            print(\"saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model training: {e}\")\n",
    "\n",
    "    def finetune(self, dataset, training_args):\n",
    "        \"\"\"\n",
    "        Execute the fine-tuning pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            dataset (Dataset): Dataset for fine-tuning.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            train_data, val_data = self.prepare_data(formatted_dataset)\n",
    "            self.train_model(train_data, val_data, training_args)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupt received, saving model...\")\n",
    "            self.model.save_pretrained(f\"{self.model_path}_interrupted\")\n",
    "            print(f\"Model saved to {self.model_path}_interrupted\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in finetuning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def lets_finetune(\n",
    "    device=DEVICE,\n",
    "    model=BASE_MODEL,\n",
    "    per_device_batch_size=4,\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=200,\n",
    "    gradient_accum_steps=4,\n",
    "):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        save_steps = 20\n",
    "        eval_steps = 20\n",
    "        max_grad_norm = 0.3\n",
    "        save_total_limit = 3\n",
    "        logging_steps = 20\n",
    "\n",
    "        print(\"\\n\" + \"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\")\n",
    "        print(\"\\033[1;34mTraining Parameters:\\033[0m\")\n",
    "        param_format = \"\\033[1;34m{:<25} {}\\033[0m\"\n",
    "        print(param_format.format(\"Foundation model:\", BASE_MODEL))\n",
    "        print(param_format.format(\"Model save path:\", MODEL_PATH))\n",
    "        print(param_format.format(\"Device used:\", DEVICE))\n",
    "        if DEVICE.type.startswith(\"xpu\"):\n",
    "            print(param_format.format(\"Intel GPU:\", torch.xpu.get_device_name()))\n",
    "        print(param_format.format(\"Batch size per device:\", per_device_batch_size))\n",
    "        print(param_format.format(\"Gradient accum. steps:\", gradient_accum_steps))\n",
    "        print(param_format.format(\"Warmup steps:\", warmup_steps))\n",
    "        print(param_format.format(\"Save steps:\", save_steps))\n",
    "        print(param_format.format(\"Evaluation steps:\", eval_steps))\n",
    "        print(param_format.format(\"Max steps:\", max_steps))\n",
    "        print(param_format.format(\"Learning rate:\", learning_rate))\n",
    "        print(param_format.format(\"Max gradient norm:\", max_grad_norm))\n",
    "        print(param_format.format(\"Save total limit:\", save_total_limit))\n",
    "        print(param_format.format(\"Logging steps:\", logging_steps))\n",
    "        print(\"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\\n\")\n",
    "\n",
    "        # Initialize the finetuner with the model and device information\n",
    "        finetuner = FineTuner(\n",
    "            base_model_id=model, model_path=MODEL_PATH, device=device\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            save_steps=save_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            #max_grad_norm=max_grad_norm,\n",
    "            bf16=True,\n",
    "            use_ipex=True,\n",
    "            #lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length=True,\n",
    "            save_total_limit=save_total_limit,\n",
    "            logging_steps=logging_steps,\n",
    "            optim=\"adamw_hf\",\n",
    "            output_dir=\"./lora_adapters\",\n",
    "            logging_dir=\"./logs\",\n",
    "        )\n",
    "        # Start fine-tuning\n",
    "        finetuner.finetune(dataset, training_args)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error occurred: name 'BitsAndBytesConfig' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;34m============================================================\u001b[0m\n",
      "\u001b[1;34mTraining Parameters:\u001b[0m\n",
      "\u001b[1;34mFoundation model:         google/gemma-7b\u001b[0m\n",
      "\u001b[1;34mModel save path:          ./final_model\u001b[0m\n",
      "\u001b[1;34mDevice used:              cpu\u001b[0m\n",
      "\u001b[1;34mBatch size per device:    4\u001b[0m\n",
      "\u001b[1;34mGradient accum. steps:    4\u001b[0m\n",
      "\u001b[1;34mWarmup steps:             20\u001b[0m\n",
      "\u001b[1;34mSave steps:               20\u001b[0m\n",
      "\u001b[1;34mEvaluation steps:         20\u001b[0m\n",
      "\u001b[1;34mMax steps:                200\u001b[0m\n",
      "\u001b[1;34mLearning rate:            2e-05\u001b[0m\n",
      "\u001b[1;34mMax gradient norm:        0.3\u001b[0m\n",
      "\u001b[1;34mSave total limit:         3\u001b[0m\n",
      "\u001b[1;34mLogging steps:            20\u001b[0m\n",
      "\u001b[1;34m============================================================\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lets_finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "INFERENCE_DEVICE = torch.device(\"cpu\")  # change this to `xpu` to use Intel GPU for inference  \n",
    "\n",
    "def generate_prompt_mermaid(input_text, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-Mermaid.js tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_text (str): The input text or question to be converted to Mermaid.js code.\n",
    "        output (str, optional): The expected Mermaid.js code as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an experienced Javascript engineer in charge of converting educational prompts into valid MermaidJS code. Make sure to only return valid MermaidJS code and nothing else. \n",
    "\n",
    "Prompt: {input_text}\n",
    "\n",
    "MermaidJS code:\n",
    "{output}\"\"\"\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(base_model_path: str):\n",
    "    \"\"\"Loads the fine-tuned model and tokenizer.\"\"\"\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        tokenizer.pad_token_id = 0\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        return model.to(INFERENCE_DEVICE), tokenizer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred during model loading: {e}\")\n",
    "        raise\n",
    "\n",
    "class TextToMermaidGenerator:\n",
    "    \"\"\"Handles Mermaid.js code generation for a given text prompt.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_model_id=BASE_MODEL, use_adapter=False, lora_checkpoint=None, loaded_base_model=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the TextToMermaidGenerator class.\n",
    "        Parameters:\n",
    "            base_model_path (str): Path to the fine-tuned model.\n",
    "            loaded_base_model (Optional[BaseModel]): Pre-loaded base model and tokenizer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if loaded_base_model:\n",
    "                self.model = loaded_base_model.model\n",
    "                self.tokenizer = loaded_base_model.tokenizer\n",
    "            else:\n",
    "                self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "            if use_adapter:\n",
    "                self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during model initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.model.to(INFERENCE_DEVICE)\n",
    "        self.max_length = 512\n",
    "\n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generates Mermaid.js code based on the given prompt.\n",
    "        Parameters:\n",
    "            prompt (str): The input prompt.\n",
    "        Returns:\n",
    "            str: The generated Mermaid.js code.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_prompt = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.to(INFERENCE_DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.cpu.amp.autocast():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=encoded_prompt,\n",
    "                        do_sample=True,\n",
    "                        max_length=self.max_length,\n",
    "                        temperature=0.3,\n",
    "                        repetition_penalty=1.2,\n",
    "                    )\n",
    "            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during Mermaid.js code generation: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vedan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m login(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_jxvvTdtkZnPJvkpuEMOORmyTvXAWQKViWG\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     11\u001b[0m BASE_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m     14\u001b[0m     BASE_MODEL, \n\u001b[0;32m     15\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Ensures authentication\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     BASE_MODEL, \n\u001b[0;32m     19\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vedan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1736\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1736\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vedan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1724\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1722\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# lets load base model for a baseline comparison\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login(\"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\") \n",
    "\n",
    "BASE_MODEL = \"google/gemma-7b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    use_auth_token=True  # Ensures authentication\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    use_auth_token=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# let's use some fake sample data\n",
    "samples = \"\"\"\n",
    "[\n",
    "{\n",
    "\"prompt\": \"How does a computer execute a program?\",\n",
    "\"expected_output\": \"flowchart LR;\\\\n    A[Start] --> B[Load Program]\\\\n    B --> C[Execute Instructions]\\\\n    C --> D[End]\"\n",
    "},\n",
    "{\n",
    "\"prompt\": \"What is the process for finding the maximum subarray sum using Kadane's Algorithm?\",\n",
    "\"expected_output\": \"flowchart LR;\\\\n    A[Start] --> B[Init max_sum, current_sum = 0]\\\\n    B --> C[For each element]\\\\n    C --> D[Add to current_sum]\\\\n    D --> E[current_sum < 0?]\\\\n    E -- Yes --> F[Reset current_sum]\\\\n    E -- No --> G[current_sum > max_sum?]\\\\n    G -- Yes --> H[Update max_sum]\\\\n    G -- No --> I[Next element]\\\\n    F --> J[End array?]\\\\n    H --> J\\\\n    I --> J\\\\n    J -- No --> C\\\\n    J -- Yes --> K[Return max_sum]\\\\n    K --> L[End]\"\n",
    "},\n",
    "{\n",
    "\"prompt\": \"How does the Quick Sort algorithm process an array to sort its elements?\",\n",
    "\"expected_output\": \"sequenceDiagram\\\\n    participant S as Start\\\\n    participant P as Select Pivot\\\\n    participant PA as Partition Array\\\\n    participant SL as Sort Left Subarray\\\\n    participant SR as Sort Right Subarray\\\\n    participant E as End\\\\n\\\\n    S->>P: Begin Sorting\\\\n    P->>PA: Choose pivot element\\\\n    PA->>SL: Partition left of pivot\\\\n    SL->>SL: Recurse left\\\\n    PA->>SR: Partition right of pivot\\\\n    SR->>SR: Recurse right\\\\n    SL->>E: Left sorted\\\\n    SR->>E: Right sorted\"\n",
    "}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def _extract_sections(output):\n",
    "    input_section = output.split(\"Prompt:\")[1].split(\"MermaidJS code:\")[0]\n",
    "    response_section = output.split(\"MermaidJS code:\")[1]\n",
    "    return input_section, response_section\n",
    "\n",
    "def run_inference(sample_data, model, finetuned=False):\n",
    "    if INFERENCE_DEVICE.type.startswith(\"xpu\"):\n",
    "        torch.xpu.empty_cache()\n",
    "        \n",
    "    print(f\"Running inference on {INFERENCE_DEVICE}...\")\n",
    "\n",
    "    color = \"#4CAF52\" if finetuned else \"#2196F4\"\n",
    "    model_type = \"finetuned\" if finetuned else \"base\"\n",
    "\n",
    "    display(HTML(f\"<div style='color:{color};'>Processing prompts on {INFERENCE_DEVICE} please wait...</div>\"))\n",
    "\n",
    "    for index, row in enumerate(sample_data):\n",
    "        try:\n",
    "            prompt = generate_prompt_mermaid(row[\"prompt\"])\n",
    "            output = model.generate(prompt)\n",
    "            input_section, response_section = _extract_sections(output)\n",
    "\n",
    "            tabbed_output = f\"\"\"\n",
    "            <details>\n",
    "            <summary style='color: {color};'><b>{model_type} model - Sample {index+1}</b> (Click to expand)</summary>\n",
    "            <div style='padding-left: 20px;'>\n",
    "            <p><b>Expected prompt 📝:</b><br>{input_section}</p>\n",
    "            <p><b>Generated Mermaid.js code 💡:</b><br><pre>{response_section}</pre></p>\n",
    "            <p><b>Expected Mermaid.js code 📈:</b><br><pre>{row[\"expected_output\"]}</pre></p>\n",
    "            </div>\n",
    "            </details>\n",
    "            <hr style='border-top: 1px solid #bbb;'>\"\"\" # Subtle separator\n",
    "\n",
    "            display(HTML(tabbed_output))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during sample processing: {e}\")\n",
    "\n",
    "# checkpoints are saved to `./lora_adapters`.\n",
    "# Update the USING_CHECKPOINT to the one you want to use.\n",
    "USING_CHECKPOINT=200\n",
    "\n",
    "# if the kernel is interrupted the latest adapter (LORA_CHECKPOINT) is `./final_model_interrupted/`\n",
    "# or else, the final model LORA_CHECKPOINT is `./final_model`\n",
    "LORA_CHECKPOINT = f\"./lora_adapters/checkpoint-{USING_CHECKPOINT}\"\n",
    "\n",
    "if os.path.exists(LORA_CHECKPOINT):\n",
    "    sample_data = json.loads(samples)\n",
    "\n",
    "    run_inference(sample_data, model=base_model)\n",
    "\n",
    "    if not finetuned_model:\n",
    "        finetuned_model = TextToMermaidGenerator(\n",
    "            base_model_id=LORA_CHECKPOINT,\n",
    "            loaded_base_model=base_model\n",
    "        )\n",
    "\n",
    "    run_inference(sample_data, model=finetuned_model, finetuned=True)\n",
    "\n",
    "# To conserve memory we can delete the model\n",
    "# del finetuned_model\n",
    "# del base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING_CHECKPOINT=200\n",
    "LORA_CHECKPOINT = f\"./lora_adapters/checkpoint-{USING_CHECKPOINT}\"\n",
    "finetuned_model, tokenizer = setup_model_and_tokenizer(BASE_MODEL)\n",
    "finetuned_model = PeftModel.from_pretrained(finetuned_model, LORA_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = finetuned_model.merge_and_unload()\n",
    "model.push_to_hub(\"Maelstrome/mermaid-gemmma-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.push_to_hub(\"Maelstrome/mermaid-gemmma-7b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
